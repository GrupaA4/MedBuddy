{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BP6sFTFJQ77S",
        "outputId": "1145fb6e-15f3-4078-8019-9732e11e0499"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.1.1-py2.py3-none-any.whl (277 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.3/277.3 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.1.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "pVQUOevrRYdT",
        "outputId": "15cd2e51-99b2-4e12-ffbe-06b6e7b3ff8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIL-Jw7bRz38",
        "outputId": "06824952-8b4d-411c-a57b-0277e896539f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.63.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "\n",
        "def build_lstm_model(input_dim, output_dim, embedding_dim, lstm_units):\n",
        "    \"\"\"\n",
        "    Construiește un model LSTM simplu.\n",
        "\n",
        "    Args:\n",
        "    input_dim (int): Dimensiunea vocabularului (numărul de cuvinte unice).\n",
        "    output_dim (int): Dimensiunea vectorului de ieșire pentru fiecare token.\n",
        "    embedding_dim (int): Dimensiunea spațiului de încorporare.\n",
        "    lstm_units (int): Numărul de unități în stratul LSTM.\n",
        "\n",
        "    Returns:\n",
        "    tf.keras.Model: Modelul LSTM compilat.\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=input_dim, output_dim=embedding_dim, input_length=None),\n",
        "        LSTM(units=lstm_units, return_sequences=True),\n",
        "        Dense(output_dim, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "wvlFE3ShR_y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual code\n"
      ],
      "metadata": {
        "id": "HtMqGlk_gnl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# THE CLASS FOR EMBEDDED WORDS WITH LABELS\n",
        "import numpy as np\n",
        "\n",
        "class LSTM:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_classes):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.num_classes = num_classes  # Number of classes possible for labeling\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Weight initialization for LSTM gates\n",
        "        self.Wf = np.random.randn(self.hidden_dim, self.input_dim + self.hidden_dim)\n",
        "        self.Wi = np.random.randn(self.hidden_dim, self.input_dim + self.hidden_dim)\n",
        "        self.Wc = np.random.randn(self.hidden_dim, self.input_dim + self.hidden_dim)\n",
        "        self.Wo = np.random.randn(self.hidden_dim, self.input_dim + self.hidden_dim)\n",
        "        self.Wy = np.random.randn(self.num_classes, self.hidden_dim)  # Output layer weights\n",
        "\n",
        "        # Bias initialization\n",
        "        self.bf = np.zeros((self.hidden_dim, 1))\n",
        "        self.bi = np.zeros((self.hidden_dim, 1))\n",
        "        self.bc = np.zeros((self.hidden_dim, 1))\n",
        "        self.bo = np.zeros((self.hidden_dim, 1))\n",
        "        self.by = np.zeros((self.num_classes, 1))\n",
        "\n",
        "    # Activation functions and their derivatives\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def tanh(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def sigmoid_derivative(self, sigmoid_output):\n",
        "        return sigmoid_output * (1 - sigmoid_output)\n",
        "\n",
        "    def tanh_derivative(self, tanh_output):\n",
        "        return 1 - np.power(tanh_output, 2)\n",
        "\n",
        "    def softmax(self, z):\n",
        "        e_z = np.exp(z - np.max(z))\n",
        "        return e_z / e_z.sum(axis=0)\n",
        "\n",
        "    def cross_entropy_loss(outputs, targets):\n",
        "        \"\"\"\n",
        "        Computes the cross-entropy loss between targets and predictions.\n",
        "\n",
        "        Args:\n",
        "        outputs: Numpy array of model outputs; these should be probabilities obtained\n",
        "                via softmax and should have shape (batch_size, sequence_length, num_classes)\n",
        "        targets: Numpy array of target labels in one-hot encoded form;\n",
        "                should have shape (batch_size, sequence_length, num_classes)\n",
        "\n",
        "        Returns:\n",
        "        float: The mean cross-entropy loss over all samples and timesteps.\n",
        "        \"\"\"\n",
        "        # Ensure numerical stability with clipping\n",
        "        epsilon = 1e-12\n",
        "        outputs = np.clip(outputs, epsilon, 1. - epsilon)\n",
        "\n",
        "        # Compute cross-entropy\n",
        "        ce_loss = -np.sum(targets * np.log(outputs)) / outputs.shape[0]\n",
        "\n",
        "        return ce_loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Performs the forward pass of the LSTM.\n",
        "\n",
        "        Args:\n",
        "        x: Input data, shape (batch_size, sequence_length, input_dim)\n",
        "\n",
        "        Returns:\n",
        "        A tuple (all_hiddens, all_outputs) where:\n",
        "        - all_hiddens is an array of hidden states for the whole sequence\n",
        "        - all_outputs is the softmax output for each word in the sequence\n",
        "        \"\"\"\n",
        "        batch_size, sequence_length, _ = x.shape\n",
        "        h_prev = np.zeros((batch_size, self.hidden_dim))\n",
        "        c_prev = np.zeros((batch_size, self.hidden_dim))\n",
        "        all_hiddens = np.zeros((batch_size, sequence_length, self.hidden_dim))\n",
        "        all_outputs = np.zeros((batch_size, sequence_length, self.num_classes))\n",
        "\n",
        "        for t in range(sequence_length):\n",
        "            xt = x[:, t, :]\n",
        "            combined = np.hstack((h_prev, xt))  # Combine the hidden state with the input vector\n",
        "\n",
        "            # LSTM gate computations\n",
        "            ft = self.sigmoid(np.dot(combined, self.Wf.T) + self.bf.T)  # Forget gate\n",
        "            it = self.sigmoid(np.dot(combined, self.Wi.T) + self.bi.T)  # Input gate\n",
        "            ct_ = self.tanh(np.dot(combined, self.Wc.T) + self.bc.T)     # Cell candidate\n",
        "            ot = self.sigmoid(np.dot(combined, self.Wo.T) + self.bo.T)  # Output gate\n",
        "\n",
        "            # Update the cell and hidden state\n",
        "            c_curr = ft * c_prev + it * ct_\n",
        "            h_curr = ot * self.tanh(c_curr)\n",
        "\n",
        "            # Store the current hidden state\n",
        "            h_prev = h_curr\n",
        "            c_prev = c_curr\n",
        "            all_hiddens[:, t, :] = h_curr\n",
        "\n",
        "            # Compute the output layer\n",
        "            output_raw = np.dot(h_curr, self.Wy.T) + self.by.T\n",
        "            all_outputs[:, t, :] = self.softmax(output_raw)\n",
        "\n",
        "        return all_hiddens, all_outputs\n",
        "\n",
        "    def backward(self, x, h, c, y, targets, dh_next, dc_next):\n",
        "        \"\"\"\n",
        "        Performs the backward pass of the LSTM.\n",
        "\n",
        "        Args:\n",
        "        x: Input data at all time steps (batch_size, sequence_length, input_dim)\n",
        "        h: Hidden states at all time steps (batch_size, sequence_length, hidden_dim)\n",
        "        c: Cell states at all time steps (batch_size, sequence_length, hidden_dim)\n",
        "        y: Outputs at all time steps (batch_size, sequence_length, output_dim)\n",
        "        targets: Ground truth for the outputs\n",
        "        dh_next: Gradient of the loss w.r.t. the next hidden state\n",
        "        dc_next: Gradient of the loss w.r.t. the next cell state\n",
        "        \"\"\"\n",
        "        batch_size, sequence_length, _ = x.shape\n",
        "\n",
        "        # Gradient accumulators for LSTM parameters\n",
        "        dWf, dWi, dWc, dWo = [np.zeros_like(w) for w in (self.Wf, self.Wi, self.Wc, self.Wo)]\n",
        "        dbf, dbi, dbc, dbo = [np.zeros_like(b) for b in (self.bf, self.bi, self.bc, self.bo)]\n",
        "        dWy = np.zeros_like(self.Wy)\n",
        "        dby = np.zeros_like(self.by)\n",
        "\n",
        "        for t in reversed(range(sequence_length)):\n",
        "            # Gradient of the output layer\n",
        "            dy = y[:, t, :] - targets[:, t, :]\n",
        "            dWy += np.dot(dy.T, h[:, t, :])\n",
        "            dby += np.sum(dy, axis=0, keepdims=True).T\n",
        "\n",
        "            # Total gradients for backpropagation\n",
        "            dh = np.dot(dy, self.Wy) + dh_next\n",
        "\n",
        "            # Output gate gradients\n",
        "            do = dh * np.tanh(c[:, t, :])\n",
        "            do = self.sigmoid_derivative(self.sigmoid(np.dot(h[:, t-1, :], self.Wo) + self.bo)) * do\n",
        "\n",
        "            # Cell state gradients\n",
        "            dc = (dc_next + dh * self.sigmoid(np.dot(h[:, t-1, :], self.Wo) + self.bo) * self.tanh_derivative(np.tanh(c[:, t, :])))\n",
        "            dc += do * self.tanh_derivative(np.tanh(c[:, t, :]))\n",
        "\n",
        "            # Input and forget gate gradients\n",
        "            di = dc * np.tanh(np.dot(h[:, t-1, :], self.Wc) + self.bc)\n",
        "            di = self.sigmoid_derivative(self.sigmoid(np.dot(h[:, t-1, :], self.Wi) + self.bi)) * di\n",
        "            df = dc * c[:, t-1, :]\n",
        "            df = self.sigmoid_derivative(self.sigmoid(np.dot(h[:, t-1, :], self.Wf) + self.bf)) * df\n",
        "\n",
        "            # Cell candidate gradients\n",
        "            dc_ = dc * self.sigmoid(np.dot(h[:, t-1, :], self.Wi) + self.bi)\n",
        "            dc_ = self.tanh_derivative(np.tanh(np.dot(h[:, t-1, :], self.Wc) + self.bc)) * dc_\n",
        "\n",
        "            # Accumulate gradients\n",
        "            dWf += np.dot(df.T, np.hstack([h[:, t-1, :], x[:, t, :]]))\n",
        "            dWi += np.dot(di.T, np.hstack([h[:, t-1, :], x[:, t, :]]))\n",
        "            dWc += np.dot(dc_.T, np.hstack([h[:, t-1, :], x[:, t, :]]))\n",
        "            dWo += np.dot(do.T, np.hstack([h[:, t-1, :], x[:, t, :]]))\n",
        "\n",
        "            dbf += np.sum(df, axis=0)\n",
        "            dbi += np.sum(di, axis=0)\n",
        "            dbc += np.sum(dc_, axis=0)\n",
        "            dbo += np.sum(do, axis=0)\n",
        "\n",
        "            # Prepare for the next step\n",
        "            dh_next = np.dot(df, self.Wf) + np.dot(di, self.Wi) + np.dot(dc_, self.Wc) + np.dot(do, self.Wo)\n",
        "            dc_next = df * c[:, t-1, :] + di * dc_ + do * np.tanh(c[:, t, :])\n",
        "\n",
        "        # Update parameters\n",
        "        self.update_weights(dWf, dWi, dWc, dWo, dbf, dbi, dbc, dbo, dWy, dby)\n",
        "\n",
        "    def update_weights(self, dWf, dWi, dWc, dWo, dbf, dbi, dbc, dbo, dWy, dby):\n",
        "        # Apply learning rate and update weights and biases\n",
        "        learning_rate = self.learning_rate\n",
        "        self.Wf -= learning_rate * dWf\n",
        "        self.Wi -= learning_rate * dWi\n",
        "        self.Wc -= learning_rate * dWc\n",
        "        self.Wo -= learning_rate * dWo\n",
        "        self.bf -= learning_rate * dbf\n",
        "        self.bi -= learning_rate * dbi\n",
        "        self.bc -= learning_rate * dbc\n",
        "        self.bo -= learning_rate * dbo\n",
        "        self.Wy -= learning_rate * dWy\n",
        "        self.by -= learning_rate * dby\n",
        "\n",
        "    def train(self, train_data, train_targets, epochs, batch_size=1, save_path=None, validation_data=None):\n",
        "        for epoch in range(epochs):\n",
        "            epoch_losses = []\n",
        "            for i in range(0, len(train_data), batch_size):\n",
        "                x = train_data[i:i+batch_size]\n",
        "                y = train_targets[i:i+batch_size]\n",
        "\n",
        "                # Assuming 'forward' returns softmax probabilities\n",
        "                _, outputs = self.forward(x)\n",
        "\n",
        "                # Calculate cross-entropy loss\n",
        "                loss = self.cross_entropy_loss(outputs, y)\n",
        "                epoch_losses.append(loss)\n",
        "\n",
        "                # Backward pass and update\n",
        "                gradients = self.backward(x, outputs, y)\n",
        "                self.update_weights(gradients)\n",
        "\n",
        "            # Average loss for the epoch\n",
        "            average_loss = np.mean(epoch_losses)\n",
        "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {average_loss}')\n",
        "\n",
        "            if save_path:\n",
        "                self.save_model(f'{save_path}_epoch_{epoch + 1}.json')\n",
        "\n",
        "            if validation_data:\n",
        "                val_loss = self.evaluate(validation_data[0], validation_data[1])\n",
        "                print(f'Validation Loss: {val_loss}')\n",
        "\n",
        "    def evaluate(self, val_data, val_targets):\n",
        "        \"\"\"\n",
        "        Evaluate the model on validation data.\n",
        "\n",
        "        Args:\n",
        "        val_data: Numpy array of validation data.\n",
        "        val_targets: Numpy array of validation targets.\n",
        "\n",
        "        Returns:\n",
        "        Float, the loss computed on the validation data.\n",
        "        \"\"\"\n",
        "        total_loss = 0\n",
        "        for i in range(len(val_data)):\n",
        "            x = val_data[i:i+1]\n",
        "            y = val_targets[i:i+1]\n",
        "            _, outputs = self.forward(x)\n",
        "            loss = np.mean((outputs - y) ** 2)\n",
        "            total_loss += loss\n",
        "        return total_loss / len(val_data)\n",
        "\n",
        "    def decode_predictions(probabilities, classes):\n",
        "        \"\"\"\n",
        "        Convert probabilities to class labels.\n",
        "\n",
        "        Args:\n",
        "        probabilities: np.array, shape (batch_size, sequence_length, num_classes)\n",
        "        classes: list, the names of the classes corresponding to the indices in the last dimension of probabilities\n",
        "\n",
        "        Returns:\n",
        "        List of lists, where each sublist contains the predicted class labels for a sequence.\n",
        "        \"\"\"\n",
        "        # Get the index of the max probability for each timestep\n",
        "        class_indices = np.argmax(probabilities, axis=-1)\n",
        "\n",
        "        # Convert indices to class labels\n",
        "        decoded_labels = [[classes[idx] for idx in sequence] for sequence in class_indices]\n",
        "        return decoded_labels\n",
        "\n",
        "    def save_model(self, filename):\n",
        "        \"\"\" Saves the model's parameters to a JSON file. \"\"\"\n",
        "        model_params = {\n",
        "            'Wf': self.Wf.tolist(), 'Wi': self.Wi.tolist(),\n",
        "            'Wc': self.Wc.tolist(), 'Wo': self.Wo.tolist(),\n",
        "            'Wy': self.Wy.tolist(),\n",
        "            'bf': self.bf.tolist(), 'bi': self.bi.tolist(),\n",
        "            'bc': self.bc.tolist(), 'bo': self.bo.tolist(),\n",
        "            'by': self.by.tolist()\n",
        "        }\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(model_params, f)\n",
        "\n",
        "    def load_model(self, filename):\n",
        "        \"\"\" Loads model parameters from a JSON file. \"\"\"\n",
        "        with open(filename, 'r') as f:\n",
        "            model_params = json.load(f)\n",
        "        self.Wf = np.array(model_params['Wf'])\n",
        "        self.Wi = np.array(model_params['Wi'])\n",
        "        self.Wc = np.array(model_params['Wc'])\n",
        "        self.Wo = np.array(model_params['Wo'])\n",
        "        self.Wy = np.array(model_params['Wy'])\n",
        "        self.bf = np.array(model_params['bf'])\n",
        "        self.bi = np.array(model_params['bi'])\n",
        "        self.bc = np.array(model_params['bc'])\n",
        "        self.bo = np.array(model_params['bo'])\n",
        "        self.by = np.array(model_params['by'])\n"
      ],
      "metadata": {
        "id": "0XTv78sgrSGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code that helped me build the actual code\n"
      ],
      "metadata": {
        "id": "ObglVKlbDMs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import math\n",
        "\n",
        "data = pd.read_csv(\"\")\n",
        "data = data.ffill()"
      ],
      "metadata": {
        "id": "7uaQwJjagq-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PREDICTORS = [\"\",\"\",\"\"] # columns to use for prediction\n",
        "TARGET = \"\" # the predicted value\n",
        "\n",
        "scaler = StandardScaler()\n",
        "data[PREDICTORS] = scaler.fit_transform(data[PREDICTORS])\n",
        "\n",
        "np.random.seed(0)\n",
        "split_data = np.split(data, [int(.7*len(data)), int(.85*len(data))])\n",
        "(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = [[d[PREDICTORS].to_numpy(), d[[TARGET]].to_numpy()] for d in split_data]"
      ],
      "metadata": {
        "id": "-TKo6ABkgxRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_params(layer_conf):\n",
        "    layers = []\n",
        "    for i in range(1, len(layer_conf)):\n",
        "        np.random.seed(0)\n",
        "        k = 1/math.sqrt(layer_conf[i][\"hidden\"])\n",
        "        i_weight = np.random.rand(layer_conf[i-1][\"units\"], layer_conf[i][\"hidden\"]) * 2 * k - k\n",
        "\n",
        "        h_weight = np.random.rand(layer_conf[i][\"hidden\"], layer_conf[i][\"hidden\"]) * 2 * k - k\n",
        "        h_bias = np.random.rand(1, layer_conf[i][\"hidden\"]) * 2 * k - k\n",
        "\n",
        "        o_weight = np.random.rand(layer_conf[i][\"hidden\"], layer_conf[i][\"output\"]) * 2 * k - k\n",
        "        o_bias = np.random.rand(1, layer_conf[i][\"output\"]) * 2 * k - k\n",
        "\n",
        "        layers.append(\n",
        "            [i_weight, h_weight, h_bias, o_weight, o_bias]\n",
        "        )\n",
        "    return layers"
      ],
      "metadata": {
        "id": "GXswXU-4hBP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(x, layers):\n",
        "    hiddens = []\n",
        "    outputs = []\n",
        "    for i in range(len(layers)):\n",
        "        i_weight, h_weight, h_bias, o_weight, o_bias = layers[i]\n",
        "        hidden = np.zeros((x.shape[0], i_weight.shape[1]))\n",
        "        output = np.zeros((x.shape[0], o_weight.shape[1]))\n",
        "        for j in range(x.shape[0]):\n",
        "            input_x = x[j,:][np.newaxis,:] @ i_weight\n",
        "            hidden_x = input_x + hidden[max(j-1,0),:][np.newaxis,:] @ h_weight + h_bias\n",
        "            # Activation.  tanh avoids outputs getting larger and larger.\n",
        "            hidden_x = np.tanh(hidden_x)\n",
        "            # Store hidden for use in backprop\n",
        "            hidden[j,:] = hidden_x\n",
        "\n",
        "            # Output layer\n",
        "            output_x = hidden_x @ o_weight + o_bias\n",
        "            output[j,:] = output_x\n",
        "        hiddens.append(hidden)\n",
        "        outputs.append(output)\n",
        "    return hiddens, outputs[-1]"
      ],
      "metadata": {
        "id": "QKSDaVvakVyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mse(actual, predicted):\n",
        "    return np.mean((actual-predicted)**2)\n",
        "\n",
        "def mse_grad(actual, predicted):\n",
        "    return (predicted - actual)"
      ],
      "metadata": {
        "id": "MkJQ1sHLkbT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward(layers, x, lr, grad, hiddens):\n",
        "    for i in range(len(layers)):\n",
        "        i_weight, h_weight, h_bias, o_weight, o_bias = layers[i]\n",
        "        hidden = hiddens[i]\n",
        "        next_h_grad = None\n",
        "        i_weight_grad, h_weight_grad, h_bias_grad, o_weight_grad, o_bias_grad = [0] * 5\n",
        "\n",
        "        for j in range(x.shape[0] - 1, -1, -1):\n",
        "            # 1,1\n",
        "            # Add newaxis in the first dimension\n",
        "            out_grad = grad[j,:][np.newaxis, :]\n",
        "\n",
        "            # Output updates\n",
        "            # 3x1 @ 1x1 = 3,1\n",
        "            # np.newaxis creates a size 1 axis, in this case transposing matrix\n",
        "            o_weight_grad += hidden[j,:][:, np.newaxis] @ out_grad\n",
        "            # 1,1\n",
        "            o_bias_grad += out_grad\n",
        "\n",
        "            # Propagate gradient to hidden unit\n",
        "            # 1,1 @ 1,3 = 1,3\n",
        "            h_grad = out_grad @ o_weight.T\n",
        "\n",
        "            if j < x.shape[0] - 1:\n",
        "                # Then we multiply the gradient by the hidden weights to pull gradient from next hidden state to current hidden state\n",
        "                hh_grad = next_h_grad @ h_weight.T\n",
        "                # Add the gradients together to combine output contribution and hidden contribution\n",
        "                h_grad += hh_grad\n",
        "\n",
        "            # Pull the gradient across the current hidden nonlinearity\n",
        "            # derivative of tanh is 1 - tanh(x) ** 2\n",
        "            # So we take the output of tanh (next hidden state), and plug in\n",
        "            tanh_deriv = 1 - hidden[j][np.newaxis,:] ** 2\n",
        "\n",
        "            # next_h_grad @ np.diag(tanh_deriv_next) multiplies each element of next_h_grad by the deriv\n",
        "            # Effect is to pull value across nonlinearity\n",
        "            h_grad = np.multiply(h_grad, tanh_deriv)\n",
        "\n",
        "            # Store to compute h grad for previous sequence position\n",
        "            next_h_grad = h_grad.copy()\n",
        "\n",
        "            # If we're not at the very beginning\n",
        "            if j > 0:\n",
        "                # 3,1 @ 1,3\n",
        "                # Multiply input from previous layer by post-nonlinearity grad at current layer\n",
        "                h_weight_grad += hidden[j-1][:, np.newaxis] @ h_grad\n",
        "                h_bias_grad += h_grad\n",
        "\n",
        "            # 3,1 (with newaxis) @ h_grad\n",
        "            i_weight_grad += x[j,:][:,np.newaxis] @ h_grad\n",
        "\n",
        "        lr = lr / x.shape[0]\n",
        "        i_weight -= i_weight_grad * lr\n",
        "        h_weight -= h_weight_grad * lr\n",
        "        h_bias -= h_bias_grad * lr\n",
        "        o_weight -= o_weight_grad * lr\n",
        "        o_bias -= o_bias_grad * lr\n",
        "        layers[i] = [i_weight, h_weight, h_bias, o_weight, o_bias]\n",
        "    return layers"
      ],
      "metadata": {
        "id": "y9VF_mkNkcvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 250\n",
        "lr = 1e-5\n",
        "\n",
        "layer_conf = [\n",
        "    {\"type\":\"input\", \"units\": 3},\n",
        "    {\"type\": \"rnn\", \"hidden\": 4, \"output\": 1}\n",
        "]\n",
        "layers = init_params(layer_conf)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    sequence_len = 7\n",
        "    epoch_loss = 0\n",
        "    for j in range(train_x.shape[0] - sequence_len):\n",
        "        seq_x = train_x[j:(j+sequence_len),]\n",
        "        seq_y = train_y[j:(j+sequence_len),]\n",
        "        hiddens, outputs = forward(seq_x, layers)\n",
        "        grad = mse_grad(seq_y, outputs)\n",
        "        params = backward(layers, seq_x, lr, grad, hiddens)\n",
        "        epoch_loss += mse(seq_y, outputs)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        sequence_len = 7\n",
        "        valid_loss = 0\n",
        "        for j in range(valid_x.shape[0] - sequence_len):\n",
        "            seq_x = valid_x[j:(j+sequence_len),]\n",
        "            seq_y = valid_y[j:(j+sequence_len),]\n",
        "            _, outputs = forward(seq_x, layers)\n",
        "            valid_loss += mse(seq_y, outputs)\n",
        "\n",
        "        print(f\"Epoch: {epoch} train loss {epoch_loss / len(train_x)} valid loss {valid_loss / len(valid_x)}\")"
      ],
      "metadata": {
        "id": "XsWHVGKVkgYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class LSTM:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        # Dimensions for input layer, hidden layer and output layer\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # Initialize weights and biases for gates\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Can be upgraded to Glorot (Xavier) initialisation or He initialisation\n",
        "        self.Wf = np.random.randn(self.hidden_dim, self.input_dim + self.hidden_dim)\n",
        "        self.Wi = np.random.randn(self.hidden_dim, self.input_dim + self.hidden_dim)\n",
        "        self.Wc = np.random.randn(self.hidden_dim, self.input_dim + self.hidden_dim)\n",
        "        self.Wo = np.random.randn(self.hidden_dim, self.input_dim + self.hidden_dim)\n",
        "\n",
        "        self.bf = np.zeros((self.hidden_dim, 1))\n",
        "        self.bi = np.zeros((self.hidden_dim, 1))\n",
        "        self.bc = np.zeros((self.hidden_dim, 1))\n",
        "        self.bo = np.zeros((self.hidden_dim, 1))\n",
        "\n",
        "    # Activation functions and their derivatives\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def tanh(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def sigmoid_derivative(self, sigmoid_output):\n",
        "        return sigmoid_output * (1 - sigmoid_output)\n",
        "\n",
        "    def tanh_derivative(self, tanh_output):\n",
        "        return 1 - np.power(tanh_output, 2)\n",
        "\n",
        "    # Forward propagation\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: Input data, shape (batch_size, sequence_length, input_dim)\n",
        "        \"\"\"\n",
        "        batch_size, sequence_length, _ = x.shape\n",
        "        h_prev = np.zeros((batch_size, self.hidden_dim))\n",
        "        c_prev = np.zeros((batch_size, self.hidden_dim))\n",
        "        all_hiddens = np.zeros((batch_size, sequence_length, self.hidden_dim))\n",
        "        all_outputs = np.zeros((batch_size, sequence_length, self.output_dim))\n",
        "\n",
        "        for t in range(sequence_length):\n",
        "            xt = x[:, t, :]\n",
        "            combined = np.hstack((h_prev, xt))  # Combines the hidden layer with the input lair\n",
        "            ft = self.sigmoid(np.dot(combined, self.Wf.T) + self.bf.T)  # Forget gate\n",
        "            it = self.sigmoid(np.dot(combined, self.Wi.T) + self.bi.T)  # Input gate\n",
        "            ct_ = np.tanh(np.dot(combined, self.Wc.T) + self.bc.T)     # Cell candidate\n",
        "            ot = self.sigmoid(np.dot(combined, self.Wo.T) + self.bo.T) # Output gate\n",
        "\n",
        "            c_curr = ft * c_prev + it * ct_  # Update cell's state\n",
        "            h_curr = ot * np.tanh(c_curr)   # Update hidden state\n",
        "\n",
        "            # Update states for the next step\n",
        "            h_prev = h_curr\n",
        "            c_prev = c_curr\n",
        "\n",
        "            # memorize the hidden state for the whole sequence\n",
        "            all_hiddens[:, t, :] = h_curr\n",
        "\n",
        "            # We can add an output state if necessary\n",
        "            # output = some_activation(np.dot(h_curr, W_output) + b_output)\n",
        "            # all_outputs[:, t, :] = output\n",
        "\n",
        "        return all_hiddens, all_outputs\n",
        "\n",
        "\n",
        "\n",
        "    def step(self, x_t, h_prev, c_prev):\n",
        "        # Combine hidden and input state for matrix calculus\n",
        "        combined = np.hstack([h_prev, x_t])\n",
        "\n",
        "        # Gates\n",
        "        ft = self.sigmoid(np.dot(self.Wf, combined) + self.bf)  # Forget gate\n",
        "        it = self.sigmoid(np.dot(self.Wi, combined) + self.bi)  # Input gate\n",
        "        ct_ = self.tanh(np.dot(self.Wc, combined) + self.bc)    # Cell's candidate(cell gate)\n",
        "        ot = self.sigmoid(np.dot(self.Wo, combined) + self.bo)  # Output gate\n",
        "\n",
        "        # Update states for the next step\n",
        "        c_curr = ft * c_prev + it * ct_\n",
        "        h_curr = ot * self.tanh(c_curr)\n",
        "\n",
        "        return h_curr, c_curr\n",
        "\n",
        "    def backward(self, x, h, c, y, targets, dh_next, dc_next):\n",
        "        \"\"\"\n",
        "        x: input data at all time steps (batch_size, sequence_length, input_dim)\n",
        "        h: hidden states at all time steps (batch_size, sequence_length, hidden_dim)\n",
        "        c: cell states at all time steps (batch_size, sequence_length, hidden_dim)\n",
        "        y: outputs at all time steps (batch_size, sequence_length, output_dim)\n",
        "        targets: ground truth for the outputs\n",
        "        dh_next: gradient of the loss w.r.t. next hidden state\n",
        "        dc_next: gradient of the loss w.r.t. next cell state\n",
        "        \"\"\"\n",
        "        batch_size, sequence_length, _ = x.shape\n",
        "\n",
        "        # Gradient accumulators\n",
        "        dWf, dWi, dWc, dWo = [np.zeros_like(W) for W in (self.Wf, self.Wi, self.Wc, self.Wo)]\n",
        "        dbf, dbi, dbc, dbo = [np.zeros_like(b) for b in (self.bf, self.bi, self.bc, self.bo)]\n",
        "\n",
        "        # Iterate over the sequence backwards\n",
        "        for t in reversed(range(sequence_length)):\n",
        "            # Compute gradients of the output layer\n",
        "            dy = (y[:, t, :] - targets[:, t, :])  # shape: (batch_size, output_dim)\n",
        "            dWhy = np.dot(dy.T, h[:, t, :])\n",
        "            dby = np.sum(dy, axis=0, keepdims=True).T\n",
        "\n",
        "            # Gradients of the gates using the final step gradients and previous gradients\n",
        "            do = dh_next * np.tanh(c[:, t, :])\n",
        "            dc = (dc_next + dh_next * self.sigmoid(self.Wo) * self.tanh_derivative(np.tanh(c[:, t, :])))\n",
        "            dc_f = dc * self.sigmoid(self.Wf)\n",
        "            di = dc * self.sigmoid(self.Wi)\n",
        "            dg = dc * self.tanh(c[:, t, :])\n",
        "\n",
        "            # Calculate gate derivatives\n",
        "            dfo = do * self.sigmoid_derivative(self.sigmoid(self.Wo))\n",
        "            dfi = di * self.sigmoid_derivative(self.sigmoid(self.Wi))\n",
        "            dfc = dg * self.tanh_derivative(self.sigmoid(self.Wc))\n",
        "            dfo_next = self.sigmoid_derivative(np.dot(x[:, t, :], self.Wo.T) + self.bo)\n",
        "            dfi_next = self.sigmoid_derivative(np.dot(x[:, t, :], self.Wi.T) + self.bi)\n",
        "            dfc_next = self.tanh_derivative(np.dot(x[:, t, :], self.Wc.T) + self.bc)\n",
        "\n",
        "            # Accumulate gradients\n",
        "            dWf += np.dot(dfo.T, np.hstack([h[:, t-1, :], x[:, t, :]]))\n",
        "            dWi += np.dot(dfi.T, np.hstack([h[:, t-1, :], x[:, t, :]]))\n",
        "            dWc += np.dot(dfc.T, np.hstack([h[:, t-1, :], x[:, t, :]]))\n",
        "            dWo += np.dot(dfo_next.T, np.hstack([h[:, t-1, :], x[:, t, :]]))\n",
        "            dbf += np.sum(dfo, axis=0)\n",
        "            dbi += np.sum(dfi, axis=0)\n",
        "            dbc += np.sum(dfc, axis=0)\n",
        "            dbo += np.sum(dfo_next, axis=0)\n",
        "\n",
        "            # Prepare for next iteration\n",
        "            dh_next = np.dot(dfo.T, self.Wf) + np.dot(dfi.T, self.Wi) + np.dot(dfc.T, self.Wc) + np.dot(dfo_next.T, self.Wo)\n",
        "            dc_next = dc_f\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.Wf -= self.learning_rate * dWf\n",
        "        self.Wi -= self.learning_rate * dWi\n",
        "        self.Wc -= self.learning_rate * dWc\n",
        "        self.Wo -= self.learning_rate * dWo\n",
        "        self.bf -= self.learning_rate * dbf\n",
        "        self.bi -= self.learning_rate * dbi\n",
        "        self.bc -= self.learning_rate * dbc\n",
        "        self.bo -= self.learning_rate * dbo\n",
        "\n",
        "        return dWhy, dby\n",
        "\n",
        "    def update_weights(self, gradients):\n",
        "        # Extract each gradient\n",
        "        dWf, dWi, dWc, dWo, dbf, dbi, dbc, dbo = gradients\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.Wf -= self.learning_rate * dWf\n",
        "        self.Wi -= self.learning_rate * dWi\n",
        "        self.Wc -= self.learning_rate * dWc\n",
        "        self.Wo -= self.learning_rate * dWo\n",
        "\n",
        "        self.bf -= self.learning_rate * dbf\n",
        "        self.bi -= self.learning_rate * dbi\n",
        "        self.bc -= self.learning_rate * dbc\n",
        "        self.bo -= self.learning_rate * dbo\n",
        "\n",
        "# lstm = LSTM(input_dim=10, hidden_dim=50, output_dim=1)\n"
      ],
      "metadata": {
        "id": "w7SWywOjowE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_lstm(model, data, epochs, learning_rate):\n",
        "    input_data, target_data = data\n",
        "    loss_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for i in range(len(input_data)):\n",
        "            # Forward propagation\n",
        "            hiddens, outputs = model.forward(input_data[i])\n",
        "\n",
        "            # Loss calculus (MSE - can be any other function)\n",
        "            loss = np.mean((outputs - target_data[i]) ** 2)\n",
        "            total_loss += loss\n",
        "\n",
        "            # Backpropagation\n",
        "            gradients = model.backward(input_data[i], hiddens, outputs, target_data[i])\n",
        "\n",
        "            # Update weights\n",
        "            model.update_weights(gradients, learning_rate)\n",
        "\n",
        "        # Mean loss per epoch\n",
        "        average_loss = total_loss / len(input_data)\n",
        "        loss_history.append(average_loss)\n",
        "        print(f'Epoch: {epoch + 1}/{epochs}, Batch: {i + 1}/{len(input_data)}, Loss: {average_loss}')\n",
        "\n",
        "    return loss_history"
      ],
      "metadata": {
        "id": "-IIG4QdCbLoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To save\n",
        "import json\n",
        "\n",
        "def save_model(self, filename):\n",
        "    model_params = {\n",
        "        'Wf': self.Wf.tolist(), 'Wi': self.Wi.tolist(),\n",
        "        'Wc': self.Wc.tolist(), 'Wo': self.Wo.tolist(),\n",
        "        'bf': self.bf.tolist(), 'bi': self.bi.tolist(),\n",
        "        'bc': self.bc.tolist(), 'bo': self.bo.tolist()\n",
        "    }\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(model_params, f)\n",
        "\n",
        "def load_model(self, filename):\n",
        "    with open(filename, 'r') as f:\n",
        "        model_params = json.load(f)\n",
        "    self.Wf = np.array(model_params['Wf'])\n",
        "    self.Wi = np.array(model_params['Wi'])\n",
        "    self.Wc = np.array(model_params['Wc'])\n",
        "    self.Wo = np.array(model_params['Wo'])\n",
        "    self.bf = np.array(model_params['bf'])\n",
        "    self.bi = np.array(model_params['bi'])\n",
        "    self.bc = np.array(model_params['bc'])\n",
        "    self.bo = np.array(model_params['bo'])\n"
      ],
      "metadata": {
        "id": "DK8jTBynSwDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "I have a stomach ache.\n",
        "\n",
        "I - 0\n",
        "have - 0\n",
        "a - 0\n",
        "stomach - B-SYM\n",
        "ache - I-SYM"
      ],
      "metadata": {
        "id": "bphtE8Bpo8vN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}